{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7t3tA3zpWoO1fuktpIV2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DirksGAN/Pytorch/blob/main/First%20steps%20with%20pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axc_0Kz28OV6"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "356_WgQt-K60"
      },
      "source": [
        "a = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(4.0, requires_grad=True)\n",
        "c = torch.tensor(5.0, requires_grad=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy6DrvcG-UM9",
        "outputId": "45b6ff25-90cb-414b-f5b7-f91cf46ee7c3"
      },
      "source": [
        "print(a, b, c)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3., requires_grad=True) tensor(4., requires_grad=True) tensor(5., requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr4GMlsU-WLw",
        "outputId": "87f12862-48fa-4b1f-8312-8bca3263e948"
      },
      "source": [
        "a2 = 3*a + 1*b + c\n",
        "print(a2)\n",
        "print(a2.backward())\n",
        "print(a2.grad)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(18., grad_fn=<AddBackward0>)\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_VMISo_XL3"
      },
      "source": [
        "import PIL as plt"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VLyyk95_cAe",
        "outputId": "d6f093b2-d548-4424-d424-f82d6f2d9770"
      },
      "source": [
        "#Gebe mir einen random tensor mit 3 Einträgen aus     \n",
        "newA = torch.rand(3)\n",
        "print(newA)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.8075, 0.0447, 0.6077])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp1iZ7GQAQ54",
        "outputId": "4afae5bb-8bfe-4a73-d269-74cd7e549d1d"
      },
      "source": [
        "# Gebe mir einen zufälligen Tensor mit zwei Dimensionen (3*10) aus\n",
        "newB = torch.rand(3,10)\n",
        "print(newB)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4170, 0.3451, 0.6725, 0.9062, 0.5858, 0.5682, 0.7795, 0.0700, 0.9441,\n",
            "         0.6155],\n",
            "        [0.6227, 0.7617, 0.1253, 0.5726, 0.1257, 0.9037, 0.9600, 0.5538, 0.6713,\n",
            "         0.6975],\n",
            "        [0.4969, 0.9091, 0.8914, 0.6702, 0.8236, 0.2931, 0.9191, 0.4690, 0.9470,\n",
            "         0.2275]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YxAVa99gAauh",
        "outputId": "ef1d8acf-97de-462e-f724-6ba8421d9d17"
      },
      "source": [
        "# Warum Pytorch verwenden statt Tensorflow / Keras?\n",
        "'''\n",
        "1.) Pytorch ist state of the art, wie man im Englischen sagt. \n",
        "2.) Der Code ist sehr einfach lesbar\n",
        "3.) Bei Tensorflow nutzt man eher Keras statt native Tensorflow\n",
        "4.) Pytorch ist von Facebook -> Von einem kompetenten Team\n",
        "5.) Pytorch ist High Level \n",
        "6.) In Tensorflow muss man immer gleiche Dateigrößen eingeben\n",
        "'''"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1.) Pytorch ist state of the art, wie man im Englischen sagt. \\n2.) Der Code ist sehr einfach lesbar\\n3.) Bei Tensorflow nutzt man eher Keras statt native Tensorflow\\n4.) Pytorch ist von Facebook -> Von einem kompetenten Team\\n5.) Pytorch ist High Level \\n6.) In Tensorflow muss man immer gleiche Dateigrößen eingeben\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbWj83ddBjak",
        "outputId": "e6d5b574-9d98-4361-c8e7-f5bd9fa5ff72"
      },
      "source": [
        "# Hier wollen wir uns mit Tensoren beschäftigen. Ein Tensor ist \"eine Variable\" in Pytorch\n",
        "# Ein Tensor ist eine Matrix\n",
        "\n",
        "meinTensor = torch.rand(2,2)*10\n",
        "print(meinTensor)\n",
        "\n",
        "mt2 = torch.tensor(5.0)\n",
        "print(mt2)\n",
        "\n",
        "mt3 = torch.tensor(3)\n",
        "print(mt3)\n",
        "\n",
        "mt4 = torch.randn(3)\n",
        "print(mt4)\n",
        "\n",
        "mt5 = torch.zeros_like(meinTensor)\n",
        "print(mt5)\n",
        "print(meinTensor)\n",
        "print(meinTensor.size())\n",
        "\n",
        "ergebnis1 = torch.add(meinTensor, mt5)\n",
        "print(ergebnis1)\n",
        "\n",
        "mt6 = torch.tensor([1,2,3,4,5])\n",
        "print(mt6)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.4447, 3.1352],\n",
            "        [2.9455, 2.6822]])\n",
            "tensor(5.)\n",
            "tensor(3)\n",
            "tensor([-0.1915, -1.2219,  0.1528])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[4.4447, 3.1352],\n",
            "        [2.9455, 2.6822]])\n",
            "torch.Size([2, 2])\n",
            "tensor([[4.4447, 3.1352],\n",
            "        [2.9455, 2.6822]])\n",
            "tensor([1, 2, 3, 4, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcBtp3FREEfJ",
        "outputId": "753444d7-a6c7-4933-8dca-774fec1fb6e5"
      },
      "source": [
        "t1 = torch.tensor([[1,2,3],[1,2,3]])\n",
        "print(t1)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  t1 = t1.cuda()\n",
        "print(t1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3]])\n",
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upgio6IzGAg1",
        "outputId": "b04e2efa-398a-4aa9-b989-19a77b4805cf"
      },
      "source": [
        "# Das erste eigene Netzwerk\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "class MeinNetz(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MeinNetz, self).__init__()\n",
        "    self.lin1 = nn.Linear(10,10)\n",
        "    self.lin2 = nn.Linear(10,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.lin1(x))\n",
        "    x = self.lin2(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:]\n",
        "    num = 1\n",
        "    for i in size:\n",
        "      num *= i\n",
        "    return num\n",
        "\n",
        "netz = MeinNetz()\n",
        "for i in range(100):\n",
        "\n",
        "  input = Variable(torch.randn(10,10))\n",
        "  out = netz(input)\n",
        "\n",
        "  x = [0,0,0,0,0,0,0,0,0,0]\n",
        "  target= Variable(torch.Tensor([x for _ in range(10)]))\n",
        "  criterion = nn.MSELoss()\n",
        "  loss = criterion(out, target)\n",
        "  print(loss)\n",
        "\n",
        "  netz.zero_grad()\n",
        "  loss.backward(retain_graph=True)\n",
        "  optimizer = optim.Adam(netz.parameters(), lr=0.01)\n",
        "  optimizer.step()\n",
        "\n",
        "  # Neuronale Netzwerke haben sehr viel mit ausprobieren zu tun.\n",
        "\n",
        "print(out)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1574, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1051, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0845, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0749, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0462, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0419, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0303, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0274, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0248, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0155, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0270, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0175, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0164, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0133, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0161, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0089, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0095, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0101, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0056, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0062, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0126, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0042, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0033, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0041, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0036, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0049, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "tensor([[-0.0481, -0.0465,  0.0004,  0.0568, -0.0167,  0.0700,  0.0144, -0.0141,\n",
            "         -0.0329, -0.0564],\n",
            "        [ 0.0165,  0.0103, -0.0193, -0.0030,  0.0074,  0.0114, -0.0075,  0.0240,\n",
            "         -0.0146, -0.0100],\n",
            "        [ 0.0270,  0.0313, -0.0030, -0.0276, -0.0164, -0.0289, -0.0223,  0.0137,\n",
            "         -0.0065, -0.0122],\n",
            "        [-0.0281, -0.0268, -0.0060,  0.0344, -0.0171, -0.0028,  0.0049, -0.0503,\n",
            "         -0.0194, -0.0430],\n",
            "        [ 0.0185,  0.0148,  0.0094, -0.0227, -0.0267, -0.0200, -0.0190,  0.0180,\n",
            "         -0.0089, -0.0138],\n",
            "        [-0.0057, -0.0188, -0.0142,  0.0084, -0.0066,  0.0104,  0.0036,  0.0008,\n",
            "         -0.0119, -0.0134],\n",
            "        [ 0.0239,  0.0173, -0.0231, -0.0217,  0.0072, -0.0311,  0.0039,  0.0166,\n",
            "          0.0054,  0.0049],\n",
            "        [-0.0070, -0.0471, -0.0155,  0.0285, -0.0217,  0.0068, -0.0312, -0.0368,\n",
            "         -0.0501, -0.0055],\n",
            "        [ 0.0081, -0.0137,  0.0085,  0.0013,  0.0060,  0.0341,  0.0254,  0.0176,\n",
            "         -0.0070, -0.0322],\n",
            "        [ 0.0425,  0.0403, -0.0168, -0.0339, -0.0023, -0.0367, -0.0330,  0.0125,\n",
            "         -0.0065,  0.0049]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}